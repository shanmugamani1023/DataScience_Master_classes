{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. What is Gradient Boosting Regression? \n",
    "Gradient Boosting Regression is a machine learning technique used for regression tasks. It works by sequentially training multiple regression models, each one correcting the errors of its predecessor. It minimizes a loss function by adding weak learners (typically decision trees) in a stepwise manner, where each tree is fitted to the residuals of the previous one. This iterative process continues until a predefined number of iterations is reached or until the model's performance plateaus. The final prediction is the sum of the predictions from all weak learners, adjusted by a learning rate parameter. Gradient Boosting Regression is known for its ability to handle complex datasets and produce highly accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a \n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's \n",
    "performance using metrics such as mean squared error and R-squared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize with the mean of the target values\n",
    "        self.initial_prediction = np.mean(y)\n",
    "        y_pred = np.full_like(y, self.initial_prediction)\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            # Calculate residuals\n",
    "            residuals = y - y_pred\n",
    "            \n",
    "            # Fit a decision tree to the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            \n",
    "            # Update predictions with a fraction of the prediction from the new tree\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "            \n",
    "            # Store the model\n",
    "            self.models.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.full(len(X), self.initial_prediction)\n",
    "        for model in self.models:\n",
    "            y_pred += self.learning_rate * model.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# Sample usage:\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Generate sample data\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train gradient boosting regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to \n",
    "optimise the performance of the model. Use grid search or random search to find the best \n",
    "hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# Initialize the gradient boosting regressor\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(gb_regressor, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters (Grid Search):\", best_params)\n",
    "\n",
    "# Initialize the gradient boosting regressor with the best hyperparameters\n",
    "best_gb_regressor_grid = GradientBoostingRegressor(**best_params)\n",
    "\n",
    "# Fit the model with the best hyperparameters\n",
    "best_gb_regressor_grid.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_best_grid = best_gb_regressor_grid.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse_best_grid = mean_squared_error(y_test, y_pred_best_grid)\n",
    "r2_best_grid = r2_score(y_test, y_pred_best_grid)\n",
    "\n",
    "print(\"Best Mean Squared Error (Grid Search):\", mse_best_grid)\n",
    "print(\"Best R-squared (Grid Search):\", r2_best_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define the parameter distributions for random search\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# Initialize the gradient boosting regressor\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Perform random search\n",
    "random_search = RandomizedSearchCV(gb_regressor, param_distributions=param_dist, n_iter=10, cv=3, scoring='neg_mean_squared_error', random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params_random = random_search.best_params_\n",
    "print(\"Best Hyperparameters (Random Search):\", best_params_random)\n",
    "\n",
    "# Initialize the gradient boosting regressor with the best hyperparameters\n",
    "best_gb_regressor_random = GradientBoostingRegressor(**best_params_random)\n",
    "\n",
    "# Fit the model with the best hyperparameters\n",
    "best_gb_regressor_random.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_best_random = best_gb_regressor_random.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse_best_random = mean_squared_error(y_test, y_pred_best_random)\n",
    "r2_best_random = r2_score(y_test, y_pred_best_random)\n",
    "\n",
    "print(\"Best Mean Squared Error (Random Search):\", mse_best_random)\n",
    "print(\"Best R-squared (Random Search):\", r2_best_random)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting? \n",
    "In Gradient Boosting, a weak learner refers to a simple, base model that performs slightly better than random guessing on a given task. Typically, decision trees with shallow depth (often referred to as \"stumps\") are used as weak learners in Gradient Boosting. These weak learners are trained sequentially, with each one focusing on correcting the errors made by its predecessors. While individual weak learners may not be very accurate on their own, the ensemble of weak learners created by Gradient Boosting can produce a strong learner with high predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm? \n",
    "The intuition behind the Gradient Boosting algorithm is to combine the predictive power of multiple weak learners (typically shallow decision trees) to create a strong learner that makes accurate predictions. Here's a simplified intuition:\n",
    "\n",
    "1. **Sequential Improvement**: Gradient Boosting works by sequentially adding weak learners to an ensemble, with each one learning from the mistakes of its predecessors.\n",
    "\n",
    "2. **Gradient Descent**: It uses gradient descent optimization to minimize a loss function (usually a differentiable loss like mean squared error) by adding weak learners that reduce the residual errors of the model.\n",
    "\n",
    "3. **Weighted Voting**: Each weak learner contributes to the final prediction with a certain weight, which is determined based on its performance in reducing the loss function. Weak learners with higher performance are given more weight in the final prediction.\n",
    "\n",
    "4. **Strong Learner**: By combining multiple weak learners, Gradient Boosting creates a strong learner that can capture complex patterns and make accurate predictions on the dataset.\n",
    "\n",
    "Overall, the intuition is to iteratively improve the model's predictions by focusing on the areas where the model performs poorly, thereby gradually reducing the prediction errors and creating a powerful ensemble model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners? \n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners through an iterative process. Here's how it typically works:\n",
    "\n",
    "1. **Initialization**: The ensemble starts with an initial prediction, often the mean of the target variable.\n",
    "\n",
    "2. **Sequential Training**: Weak learners, typically decision trees, are added to the ensemble one at a time. Each weak learner is trained to predict the residuals (or errors) of the ensemble's current prediction.\n",
    "\n",
    "3. **Residual Calculation**: At each iteration, the residuals are calculated as the differences between the actual target values and the current ensemble prediction.\n",
    "\n",
    "4. **Training Weak Learners**: The weak learner is trained on the dataset, focusing on predicting the residuals. It learns to capture the patterns in the data that are not yet captured by the ensemble.\n",
    "\n",
    "5. **Adding to Ensemble**: Once trained, the weak learner's predictions are added to the ensemble with a certain weight. This weight is determined by a learning rate parameter and is optimized to minimize the loss function.\n",
    "\n",
    "6. **Iterative Improvement**: The process continues for a predefined number of iterations or until a stopping criterion is met. Each weak learner is added to the ensemble to correct the errors of the previous ones, gradually improving the overall prediction accuracy.\n",
    "\n",
    "7. **Final Prediction**: The final prediction is made by summing up the predictions of all weak learners in the ensemble, often weighted by their individual contributions.\n",
    "\n",
    "By iteratively adding weak learners that focus on the remaining errors of the ensemble, Gradient Boosting builds a strong learner that can capture complex patterns in the data and make accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting \n",
    "algorithm?\n",
    "Constructing the mathematical intuition of the Gradient Boosting algorithm involves understanding the underlying principles of gradient descent optimization and ensemble learning. Here are the key steps involved:\n",
    "\n",
    "1. **Loss Function Selection**: Choose an appropriate loss function to optimize during the training process. Common choices include mean squared error (MSE) for regression tasks and cross-entropy loss for classification tasks.\n",
    "\n",
    "2. **Initialization**: Start with an initial prediction, often the mean of the target variable for regression tasks or a class probability for classification tasks.\n",
    "\n",
    "3. **Residual Calculation**: Calculate the residuals by taking the differences between the actual target values and the current prediction.\n",
    "\n",
    "4. **Training Weak Learners**: Train a weak learner, typically a decision tree, to predict the residuals. The objective is to minimize the loss function with respect to the residuals.\n",
    "\n",
    "5. **Gradient Descent Optimization**: Use gradient descent optimization to update the ensemble's prediction by adding a fraction of the prediction from the weak learner. This fraction is determined by a learning rate parameter and is optimized to minimize the loss function.\n",
    "\n",
    "6. **Iterative Improvement**: Iterate the process by calculating new residuals based on the updated ensemble prediction and training additional weak learners to predict these residuals. Each weak learner is added to the ensemble to correct the errors made by the previous ones.\n",
    "\n",
    "7. **Stopping Criterion**: Decide when to stop adding weak learners to the ensemble, typically based on a predefined number of iterations or when the improvement in performance reaches a threshold.\n",
    "\n",
    "8. **Final Prediction**: Make the final prediction by summing up the predictions of all weak learners in the ensemble, often weighted by their individual contributions.\n",
    "\n",
    "By following these steps, the Gradient Boosting algorithm constructs an ensemble of weak learners that collectively produce a strong learner capable of accurately predicting the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
